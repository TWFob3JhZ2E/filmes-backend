import os
import json
import logging
import asyncio
import aiohttp
import requests
from bs4 import BeautifulSoup
from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS
from flask_jwt_extended import JWTManager, jwt_required, create_access_token
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
from threading import Thread, Lock
from urllib.parse import urljoin

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('superflix_api.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

app = Flask(__name__, static_folder='static')
CORS(app, resources={r"/*": {"origins": ["https://filmes-frontend.vercel.app", "http://localhost:3000"]}})
app.config['JWT_SECRET_KEY'] = os.getenv('JWT_SECRET_KEY', 'Xj9kL2mPqRtYvWzBHSJKDHAKJSDcNfHsD2343hhhHSKAStM4U.')
jwt = JWTManager(app)
limiter = Limiter(
    app,
    key_func=get_remote_address,
    default_limits=["200 per day", "50 per hour"],
    storage_uri=os.getenv('redis://red-d0f3prs9c44c73cav8ag:6379', 'memory://')
)

# Lock para sincronizar acesso a arquivos JSON
json_lock = Lock()

# Configura√ß√µes centralizadas
CONFIG = {
    'BASE_URL': 'https://superflixapi.nexus/filmes',
    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'ITEMS_PER_PAGE': 50,
    'JSON_INDENT': 4,
    'BASE_DIR': os.path.abspath(os.path.join(os.path.dirname(__file__), '..')),
    'TEMP_DIR': 'temp',
    'FILMES_ENCONTRADOS_DIR': 'Filmes_Encontrados',
    'RATE_LIMIT_REQUESTS': 5,
    'RATE_LIMIT_PERIOD': 1.0
}

# Caminhos para diret√≥rios
TEMP_DIR = os.path.join(CONFIG['BASE_DIR'], CONFIG['TEMP_DIR'])
FILMES_ENCONTRADOS_DIR = os.path.join(CONFIG['BASE_DIR'], CONFIG['FILMES_ENCONTRADOS_DIR'])

# Garante que os diret√≥rios existem
os.makedirs(TEMP_DIR, exist_ok=True)
os.makedirs(FILMES_ENCONTRADOS_DIR, exist_ok=True)

# Caminhos para arquivos JSON
JSON_PATHS = {
    'filmes_pagina': os.path.join(FILMES_ENCONTRADOS_DIR, 'CodeFilmesNomes.json'),
    'series_nomes': os.path.join(FILMES_ENCONTRADOS_DIR, 'CodeSeriesNomes.json'),
    'filmes_novos': os.path.join(TEMP_DIR, 'Novosfilmes.json'),
    'series': os.path.join(TEMP_DIR, 'series.json'),
    'filmes_home': os.path.join(TEMP_DIR, 'Filmes.json'),
    'code_filmes': os.path.join(TEMP_DIR, 'CodeFilmes.json'),
    'code_series': os.path.join(TEMP_DIR, 'CodeSeries.json')
}

@app.before_request
def block_suspicious_user_agents():
    """Bloqueia requisi√ß√µes de User-Agents suspeitos."""
    user_agent = request.headers.get('User-Agent', '').lower()
    blocked_agents = ['python-requests', 'curl', 'bot', 'spider', 'scanner', 'googlebot', 'bingbot']
    if any(agent in user_agent for agent in blocked_agents):
        logger.warning(f"Requisi√ß√£o bloqueada de User-Agent: {user_agent}")
        return jsonify({"erro": "Acesso n√£o autorizado"}), 403

@app.errorhandler(404)
def not_found(error):
    return jsonify({"erro": "Recurso n√£o dispon√≠vel"}), 404

@app.errorhandler(500)
def server_error(error):
    logger.error(f"Erro interno: {str(error)}")
    return jsonify({"erro": "Erro interno do servidor"}), 500

def carregar_dados_json(caminho):
    """Carrega dados de um arquivo JSON com sincroniza√ß√£o."""
    with json_lock:
        if os.path.exists(caminho):
            try:
                with open(caminho, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except json.JSONDecodeError as e:
                logger.error(f"Erro ao decodificar {caminho}: {e}")
                return []
        logger.warning(f"Arquivo {caminho} n√£o encontrado")
        return []

def salvar_dados_json(caminho, dados):
    """Salva dados em um arquivo JSON com sincroniza√ß√£o."""
    with json_lock:
        try:
            with open(caminho, 'w', encoding='utf-8') as f:
                json.dump(dados, f, ensure_ascii=False, indent=CONFIG['JSON_INDENT'])
            logger.info(f"Arquivo {caminho} salvo com sucesso")
        except Exception as e:
            logger.error(f"Erro ao salvar {caminho}: {e}")

def item_existe(lista, item_id):
    """Verifica se um item com o ID existe na lista."""
    return any(item.get('id') == item_id for item in lista)

async def extrair_detalhes_item(session, url_detalhes, semaphore):
    """Extrai detalhes adicionais de um filme ou s√©rie a partir da p√°gina de detalhes (ass√≠ncrono)."""
    async with semaphore:
        try:
            headers = {'User-Agent': CONFIG['USER_AGENT']}
            async with session.get(url_detalhes, headers=headers) as response:
                response.raise_for_status()
                content = await response.text()

            soup = BeautifulSoup(content, 'html.parser')

            titulo_original_elem = soup.find('span', class_='original-title') or soup.find('h2', class_='original-title')
            titulo_original = titulo_original_elem.get_text(strip=True) if titulo_original_elem else None

            descricao_elem = soup.find('div', class_='description') or soup.find('p', class_='synopsis')
            descricao = descricao_elem.get_text(strip=True) if descricao_elem else ""

            generos_elem = soup.find('div', class_='genres') or soup.find('ul', class_='genres-list')
            generos = []
            if generos_elem:
                generos = [g.get_text(strip=True) for g in generos_elem.find_all(['span', 'li'])]
            generos = generos if generos else []

            await asyncio.sleep(CONFIG['RATE_LIMIT_PERIOD'] / CONFIG['RATE_LIMIT_REQUESTS'])

            return {
                'titulo_original': titulo_original,
                'descricao': descricao,
                'generos': generos
            }
        except aiohttp.ClientError as e:
            logger.error(f"Erro ao extrair detalhes de {url_detalhes}: {e}")
            return {
                'titulo_original': None,
                'descricao': "",
                'generos': []
            }

async def atualizar_dados(url, cache_path, tipo='filmes'):
    """Fun√ß√£o gen√©rica para atualizar filmes ou s√©ries via scraping ass√≠ncrono."""
    cache = carregar_dados_json(cache_path)
    try:
        async with aiohttp.ClientSession() as session:
            headers = {'User-Agent': CONFIG['USER_AGENT']}
            async with session.get(url, headers=headers) as response:
                response.raise_for_status()
                content = await response.text()

            soup = BeautifulSoup(content, 'html.parser')
            novos_itens = []
            detalhes_tasks = []

            semaphore = asyncio.Semaphore(CONFIG['RATE_LIMIT_REQUESTS'])

            for poster in soup.find_all('div', class_='poster'):
                try:
                    titulo = poster.find('span', class_='title')
                    qualidade = poster.find('span', class_='year')
                    imagem = poster.find('img')
                    link = poster.find('a', class_='btn')

                    if not all([titulo, qualidade, imagem, link]):
                        continue

                    titulo = titulo.get_text(strip=True)
                    qualidade = qualidade.get_text(strip=True)
                    imagem = urljoin(CONFIG['BASE_URL'], imagem['src'])
                    item_id = link['href'].split('/')[-1]
                    url_detalhes = urljoin(CONFIG['BASE_URL'], link['href'])

                    if not item_existe(cache, item_id):
                        detalhes_tasks.append(extrair_detalhes_item(session, url_detalhes, semaphore))
                        novos_itens.append({
                            'titulo': titulo,
                            'qualidade': qualidade,
                            'capa': imagem,
                            'id': item_id
                        })
                except (AttributeError, KeyError) as e:
                    logger.warning(f"Erro ao processar item em {url}: {e}")
                    continue

            if novos_itens:
                detalhes_results = await asyncio.gather(*detalhes_tasks, return_exceptions=True)
                for i, detalhes in enumerate(detalhes_results):
                    if isinstance(detalhes, dict):
                        novos_itens[i].update({
                            'titulo_original': detalhes['titulo_original'],
                            'descricao': detalhes['descricao'],
                            'generos': detalhes['generos']
                        })

                cache.extend(novos_itens)
                salvar_dados_json(cache_path, cache)
                logger.info(f"{len(novos_itens)} novos {tipo} adicionados ao cache")
            else:
                logger.info(f"Nenhum novo {tipo} encontrado")

    except aiohttp.ClientError as e:
        logger.error(f"Erro ao atualizar {tipo} de {url}: {e}")

def run_async_in_thread(coro):
    """Executa uma corrotina ass√≠ncrona em uma thread."""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        loop.run_until_complete(coro)
    finally:
        loop.close()

def validar_id(item_id):
    """Valida se o ID √© alfanum√©rico e n√£o vazio."""
    return item_id and item_id.isalnum()

def validar_pagina(pagina):
    """Valida e converte o n√∫mero da p√°gina."""
    try:
        return max(1, int(pagina))
    except (ValueError, TypeError):
        return 1

@app.route('/login', methods=['POST'])
def login():
    """Gera um token JWT para autentica√ß√£o."""
    if not request.is_json:
        return jsonify({"erro": "JSON requerido"}), 400
    username = request.json.get('username')
    password = request.json.get('password')
    if username == 'admin' and password == 'sua-senha-secreta':
        return jsonify({"access_token": create_access_token(identity=username)})
    return jsonify({"erro": "Credenciais inv√°lidas"}), 401

@app.route('/')
@jwt_required()
@limiter.limit("10 per minute")
def home():
    """Endpoint inicial da API."""
    return jsonify({"mensagem": "API Superflix est√° online üöÄ"})

@app.route('/filme/detalhes')
@jwt_required()
@limiter.limit("10 per minute")
def filme_detalhes():
    """Retorna detalhes de um filme pelo ID."""
    filme_id = request.args.get('id')
    if not validar_id(filme_id):
        logger.warning(f"ID de filme inv√°lido: {filme_id}")
        return jsonify({'erro': 'Recurso n√£o dispon√≠vel'}), 400

    filmes = carregar_dados_json(JSON_PATHS['filmes_pagina'])
    for filme in filmes:
        if filme.get('id') == filme_id:
            return jsonify(filme)

    logger.info(f"Filme com ID {filme_id} n√£o encontrado")
    return jsonify({'erro': 'Recurso n√£o dispon√≠vel'}), 404

@app.route('/serie/detalhes')
@jwt_required()
@limiter.limit("10 per minute")
def serie_detalhes():
    """Retorna detalhes de uma s√©rie pelo ID."""
    serie_id = request.args.get('id')
    if not validar_id(serie_id):
        logger.warning(f"ID de s√©rie inv√°lido: {serie_id}")
        return jsonify({'erro': 'Recurso n√£o dispon√≠vel'}), 400

    series = carregar_dados_json(JSON_PATHS['series_nomes'])
    for serie in series:
        if serie.get('id') == serie_id:
            return jsonify(serie)

    logger.info(f"S√©rie com ID {serie_id} n√£o encontrada")
    return jsonify({'erro': 'Recurso n√£o dispon√≠vel'}), 404

@app.route('/codigos/series')
@jwt_required()
@limiter.limit("5 per minute")
def codigos_series():
    """Retorna c√≥digos de s√©ries, com cache."""
    cache = carregar_dados_json(JSON_PATHS['code_series'])
    if cache:
        return jsonify({"codigos": ", ".join(cache.get("codigos", []))})

    try:
        url = urljoin(CONFIG['BASE_URL'], '/series/lista/')
        headers = {'User-Agent': CONFIG['USER_AGENT']}
        response = requests.get(url, headers=headers, timeout=5)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')
        raw_codigos = soup.decode_contents().split('<br/>')
        codigos = [codigo.strip() for codigo in raw_codigos if codigo.strip().isdigit()]

        cache = {"codigos": codigos}
        salvar_dados_json(JSON_PATHS['code_series'], cache)
        logger.info("C√≥digos de s√©ries atualizados")
        return jsonify({"codigos": ", ".join(codigos)})

    except requests.exceptions.RequestException as e:
        logger.error(f"Erro ao carregar c√≥digos de s√©ries: {e}")
        return jsonify({'erro': 'Erro interno do servidor'}), 500

@app.route('/codigos/filmes')
@jwt_required()
@limiter.limit("5 per minute")
def codigos_filmes():
    """Retorna c√≥digos de filmes, com cache."""
    cache = carregar_dados_json(JSON_PATHS['code_filmes'])
    if cache:
        return jsonify({"codigos": ", ".join(cache.get("codigos", []))})

    try:
        url = urljoin(CONFIG['BASE_URL'], '/filmes/lista/')
        headers = {'User-Agent': CONFIG['USER_AGENT']}
        response = requests.get(url, headers=headers, timeout=5)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')
        dados = soup.get_text()
        import re
        codigos = re.findall(r'tt\d+', dados)

        cache = {"codigos": codigos}
        salvar_dados_json(JSON_PATHS['code_filmes'], cache)
        logger.info("C√≥digos de filmes atualizados")
        return jsonify({"codigos": ", ".join(codigos)})

    except requests.exceptions.RequestException as e:
        logger.error(f"Erro ao carregar c√≥digos de filmes: {e}")
        return jsonify({'erro': 'Erro interno do servidor'}), 500

@app.route('/filmes/novos')
@jwt_required()
@limiter.limit("5 per minute")
def filmes_novos():
    """Retorna novos filmes, com atualiza√ß√£o em segundo plano."""
    wait = request.args.get('wait', 'false').lower() == 'true'
    cache = carregar_dados_json(JSON_PATHS['filmes_novos'])

    thread = Thread(
        target=run_async_in_thread,
        args=(atualizar_dados(urljoin(CONFIG['BASE_URL'], '/filmes'), JSON_PATHS['filmes_novos'], 'filmes'),)
    )
    thread.start()

    if wait:
        thread.join()
        cache = carregar_dados_json(JSON_PATHS['filmes_novos'])

    return jsonify(cache)

@app.route('/filmes/home')
@jwt_required()
@limiter.limit("10 per minute")
def filmes_home():
    """Retorna filmes da p√°gina inicial."""
    cache = carregar_dados_json(JSON_PATHS['filmes_home'])
    return jsonify(cache)

@app.route('/filmes/pagina')
@jwt_required()
@limiter.limit("10 per minute")
def filmes_pagina():
    """Retorna filmes paginados com metadados."""
    pagina = validar_pagina(request.args.get('pagina', 1))
    cache = carregar_dados_json(JSON_PATHS['filmes_pagina'])

    inicio = (pagina - 1) * CONFIG['ITEMS_PER_PAGE']
    fim = inicio + CONFIG['ITEMS_PER_PAGE']
    filmes_paginados = cache[inicio:fim]

    total_itens = len(cache)
    total_paginas = (total_itens + CONFIG['ITEMS_PER_PAGE'] - 1) // CONFIG['ITEMS_PER_PAGE']

    return jsonify({
        'filmes': filmes_paginados,
        'total_itens': total_itens,
        'total_paginas': total_paginas,
        'pagina_atual': pagina
    })

@app.route('/filmes/pagina/atualizar')
@jwt_required()
@limiter.limit("5 per minute")
def filmes_pagina_atualizar():
    """Atualiza a lista de filmes em segundo plano."""
    wait = request.args.get('wait', 'false').lower() == 'true'
    cache = carregar_dados_json(JSON_PATHS['filmes_pagina'])

    thread = Thread(
        target=run_async_in_thread,
        args=(atualizar_dados(urljoin(CONFIG['BASE_URL'], '/filmes'), JSON_PATHS['filmes_pagina'], 'filmes'),)
    )
    thread.start()

    if wait:
        thread.join()
        cache = carregar_dados_json(JSON_PATHS['filmes_pagina'])

    return jsonify(cache)

@app.route('/series/pagina')
@jwt_required()
@limiter.limit("10 per minute")
def series_pagina():
    """Retorna s√©ries paginadas com metadados."""
    pagina = validar_pagina(request.args.get('pagina', 1))
    cache = carregar_dados_json(JSON_PATHS['series_nomes'])

    inicio = (pagina - 1) * CONFIG['ITEMS_PER_PAGE']
    fim = inicio + CONFIG['ITEMS_PER_PAGE']
    series_paginadas = cache[inicio:fim]

    total_itens = len(cache)
    total_paginas = (total_itens + CONFIG['ITEMS_PER_PAGE'] - 1) // CONFIG['ITEMS_PER_PAGE']

    return jsonify({
        'series': series_paginadas,
        'total_itens': total_itens,
        'total_paginas': total_paginas,
        'pagina_atual': pagina
    })

@app.route('/series')
@jwt_required()
@limiter.limit("5 per minute")
def series():
    """Retorna s√©ries, com atualiza√ß√£o em segundo plano."""
    wait = request.args.get('wait', 'false').lower() == 'true'
    cache = carregar_dados_json(JSON_PATHS['series'])

    thread = Thread(
        target=run_async_in_thread,
        args=(atualizar_dados(urljoin(CONFIG['BASE_URL'], '/series'), JSON_PATHS['series'], 's√©ries'),)
    )
    thread.start()

    if wait:
        thread.join()
        cache = carregar_dados_json(JSON_PATHS['series'])

    return jsonify(cache)

@app.route('/buscar')
@jwt_required()
@limiter.limit("10 per minute")
def buscar_nomes():
    """Busca filmes e s√©ries por termo."""
    termo = request.args.get('q', '').lower()
    if not termo or len(termo) < 2:
        logger.warning(f"Termo de busca inv√°lido: {termo}")
        return jsonify({'erro': 'Recurso n√£o dispon√≠vel'}), 400

    filmes = carregar_dados_json(JSON_PATHS['filmes_pagina'])
    series = carregar_dados_json(JSON_PATHS['series_nomes'])

    resultados_filmes = [f for f in filmes if termo in f.get('titulo', '').lower()]
    resultados_series = [s for s in series if termo in s.get('titulo', '').lower()]
    resultados = resultados_filmes + resultados_series

    return jsonify(resultados[:10])

@app.route('/buscar_por_genero')
@jwt_required()
@limiter.limit("10 per minute")
def buscar_por_genero():
    """Busca filmes e s√©ries por g√™nero, com pagina√ß√£o."""
    genero = request.args.get('genero', '').lower()
    pagina = validar_pagina(request.args.get('pagina', 1))

    if not genero:
        logger.warning("G√™nero n√£o fornecido")
        return jsonify({'erro': 'Recurso n√£o dispon√≠vel'}), 400

    filmes = carregar_dados_json(JSON_PATHS['filmes_pagina'])
    series = carregar_dados_json(JSON_PATHS['series_nomes'])

    for filme in filmes:
        if 'generos' not in filme or not filme['generos']:
            logger.warning(f"Filme sem g√™neros: {filme.get('titulo', 'Desconhecido')} (ID: {filme.get('id', 'N/A')})")
    for serie in series:
        if 'generos' not in serie or not serie['generos']:
            logger.warning(f"S√©rie sem g√™neros: {serie.get('titulo', 'Desconhecido')} (ID: {serie.get('id', 'N/A')})")

    resultados_filmes = [
        filme for filme in filmes
        if isinstance(filme.get('generos'), list) and any(genero.lower() in g.lower() for g in filme.get('generos', []))
    ]
    resultados_series = [
        serie for serie in series
        if isinstance(serie.get('generos'), list) and any(genero.lower() in g.lower() for g in serie.get('generos', []))
    ]

    resultados = resultados_filmes + resultados_series

    if not resultados:
        logger.info(f"Nenhum filme ou s√©rie encontrado para o g√™nero: {genero}")
        return jsonify({
            'mensagem': 'Recurso n√£o dispon√≠vel',
            'resultados': [],
            'total': 0,
            'total_paginas': 0,
            'pagina_atual': pagina
        }), 200

    inicio = (pagina - 1) * CONFIG['ITEMS_PER_PAGE']
    fim = inicio + CONFIG['ITEMS_PER_PAGE']
    resultados_paginados = resultados[inicio:fim]

    total_itens = len(resultados)
    total_paginas = (total_itens + CONFIG['ITEMS_PER_PAGE'] - 1) // CONFIG['ITEMS_PER_PAGE']

    return jsonify({
        'resultados': resultados_paginados,
        'total': total_itens,
        'total_paginas': total_paginas,
        'pagina_atual': pagina
    })

@app.route('/buscar_generos')
@jwt_required()
@limiter.limit("10 per minute")
def buscar_generos():
    """Retorna sugest√µes de g√™neros com base no termo de busca."""
    termo = request.args.get('q', '').lower()
    generos = [
        "Action", "Animation", "Adventure", "Comedy", "Crime", "Drama", "Family",
        "Fantasy", "Western", "Science", "war", "History",
        "Lan√ßamentos", "Mystery", "Music", "Nacional", "Romance", "Suspense", "Horror",
    ]

    if not termo:
        return jsonify(generos)

    sugestoes = [g for g in generos if termo in g.lower()]
    return jsonify(sugestoes)

@app.route('/<path:path>')
@limiter.limit("10 per minute")
def serve_static(path):
    """Serve arquivos est√°ticos."""
    return send_from_directory(app.static_folder, path)

def atualizar_codigos_inicial():
    """Atualiza c√≥digos de filmes e s√©ries na inicializa√ß√£o e pr√©-carrega filmes populares."""
    with app.app_context():
        codigos_filmes()
        codigos_series()
        run_async_in_thread(
            atualizar_dados(
                urljoin(CONFIG['BASE_URL'], '/filmes'),
                JSON_PATHS['filmes_pagina'],
                'filmes'
            )
        )
        run_async_in_thread(
            atualizar_dados(
                urljoin(CONFIG['BASE_URL'], '/series'),
                JSON_PATHS['series_nomes'],
                's√©ries'
            )
        )
        logger.info("C√≥digos iniciais e filmes/s√©ries populares atualizados")

if __name__ == '__main__':
    if os.getenv('FLASK_ENV') == 'production':
        raise RuntimeError("Modo debug n√£o permitido em produ√ß√£o")
    atualizar_codigos_inicial()
    app.run(debug=True, port=5001)